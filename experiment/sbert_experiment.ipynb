{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1_id</th>\n",
       "      <th>text2_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sebuah sepeda motor diparkir di dekat dinding ...</td>\n",
       "      <td>Sebuah sepeda motor diparkir oleh mural sebuah...</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dia menikahimu, memilih untuk memiliki anak be...</td>\n",
       "      <td>mereka tidak pernah mengangkat masalah moral a...</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wanita yang meninggal itu juga mengenakan cinc...</td>\n",
       "      <td>Seorang wanita berambut pirang mengenakan arlo...</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kedua komponen harus berada di jalur tertutup.</td>\n",
       "      <td>bohlam dan baterai berada di jalur tertutup</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seperti yang sudah saya jelaskan pada bacaan k...</td>\n",
       "      <td>Seperti yang telah saya katakan dalam bacaan k...</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>PM Turki mendesak untuk mengakhiri protes di I...</td>\n",
       "      <td>Polisi Turki menembakkan gas air mata ke pengu...</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>Karena tegangannya tidak mencapai bohlam.</td>\n",
       "      <td>jalannya tidak tertutup</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>Mereka kemudian jatuh dan telah mendukung sera...</td>\n",
       "      <td>Kedua negara yang menginvasi kemudian jatuh, d...</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>Lem underlayment ke beton: berapa lama waktu y...</td>\n",
       "      <td>Berapa lama waktu yang dibutuhkan untuk menyem...</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>Asosiasi Industri Rekaman Amerika mengatakan a...</td>\n",
       "      <td>Yaitu, jika Asosiasi Industri Rekaman Amerika ...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12901 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1_id  \\\n",
       "0     Sebuah sepeda motor diparkir di dekat dinding ...   \n",
       "1     dia menikahimu, memilih untuk memiliki anak be...   \n",
       "2     Wanita yang meninggal itu juga mengenakan cinc...   \n",
       "3        Kedua komponen harus berada di jalur tertutup.   \n",
       "4     Seperti yang sudah saya jelaskan pada bacaan k...   \n",
       "...                                                 ...   \n",
       "2575  PM Turki mendesak untuk mengakhiri protes di I...   \n",
       "2576          Karena tegangannya tidak mencapai bohlam.   \n",
       "2577  Mereka kemudian jatuh dan telah mendukung sera...   \n",
       "2578  Lem underlayment ke beton: berapa lama waktu y...   \n",
       "2579  Asosiasi Industri Rekaman Amerika mengatakan a...   \n",
       "\n",
       "                                               text2_id  score  \n",
       "0     Sebuah sepeda motor diparkir oleh mural sebuah...   0.68  \n",
       "1     mereka tidak pernah mengangkat masalah moral a...   0.10  \n",
       "2     Seorang wanita berambut pirang mengenakan arlo...   0.52  \n",
       "3           bohlam dan baterai berada di jalur tertutup   0.76  \n",
       "4     Seperti yang telah saya katakan dalam bacaan k...   0.95  \n",
       "...                                                 ...    ...  \n",
       "2575  Polisi Turki menembakkan gas air mata ke pengu...   0.56  \n",
       "2576                            jalannya tidak tertutup   0.36  \n",
       "2577  Kedua negara yang menginvasi kemudian jatuh, d...   0.70  \n",
       "2578  Berapa lama waktu yang dibutuhkan untuk menyem...   0.60  \n",
       "2579  Yaitu, jika Asosiasi Industri Rekaman Amerika ...   0.35  \n",
       "\n",
       "[12901 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"dataset/sts-indonesia/train.tsv\", sep=\"\\t\")\n",
    "df_train = df_train[[\"text1_id\", \"text2_id\", \"score\"]]\n",
    "df_train[\"score\"] = df_train[\"score\"] / 5.0\n",
    "df_train\n",
    "\n",
    "df_test = pd.read_csv(\"dataset/sts-indonesia/test.tsv\", sep=\"\\t\")\n",
    "df_test = df_test[[\"text1_id\", \"text2_id\", \"score\"]]\n",
    "df_test[\"score\"] = df_test[\"score\"] / 5.0\n",
    "df_test\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDOBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Iteration: 100%|██████████| 646/646 [3:14:13<00:00, 18.04s/it]\n",
      "Epoch: 100%|██████████| 1/1 [3:15:04<00:00, 11704.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45549373476299"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('indolem/indobert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "train_examples = []\n",
    "for i, row in df_train.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "test_examples = []\n",
    "for i, row in df_test.iterrows():\n",
    "    test_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator)\n",
    "model.save(\"model/indobert-base-uncased\", model_name=\"indobert-base-uncased\")\n",
    "\n",
    "model.evaluate(evaluator)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT BASE UNCASED + DENSE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Iteration: 100%|██████████| 646/646 [41:15<00:00,  3.83s/it]\n",
      "Epoch: 100%|██████████| 1/1 [42:36<00:00, 2556.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6966998906191842"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "train_examples = []\n",
    "for i, row in df_train.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "test_examples = []\n",
    "for i, row in df_test.iterrows():\n",
    "    test_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator)\n",
    "model.save(\"model/bert-base-uncased-dense\", model_name=\"bert-base-uncased-dense\")\n",
    "model.evaluate(evaluator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT BASED CASED + DENSE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Iteration: 100%|██████████| 646/646 [2:40:46<00:00, 14.93s/it]\n",
      "Epoch: 100%|██████████| 1/1 [2:42:18<00:00, 9738.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6806033636732052"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('bert-base-cased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "train_examples = []\n",
    "for i, row in df_train.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "test_examples = []\n",
    "for i, row in df_test.iterrows():\n",
    "    test_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator)\n",
    "model.save(\"model/bert-base-cased-dense\", model_name=\"bert-base-cased-dense\")\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples)\n",
    "model.evaluate(evaluator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDOBERT + DENSE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Iteration: 100%|██████████| 646/646 [19:33<00:00,  1.82s/it]\n",
      "Epoch: 100%|██████████| 1/1 [20:27<00:00, 1227.91s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7783104346209084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('indolem/indobert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "train_examples = []\n",
    "for i, row in df_train.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "test_examples = []\n",
    "for i, row in df_test.iterrows():\n",
    "    test_examples.append(InputExample(texts=[row[\"text1_id\"], row[\"text2_id\"]], label=row[\"score\"]))\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator)\n",
    "model.save(\"model/indobert-base-uncased-dense\", model_name=\"indobert-base-uncased-dense\")\n",
    "\n",
    "model.evaluate(evaluator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:13:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
